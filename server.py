from flask import Flask, request, jsonify, send_from_directory
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import requests
from bs4 import BeautifulSoup
import re
import time
from flask_cors import CORS
import os
import ssl

app = Flask(__name__, static_folder="static", static_url_path="")
CORS(app) 

try:
    model = AutoModelForTokenClassification.from_pretrained("./ner_model/checkpoint-125")
    tokenizer = AutoTokenizer.from_pretrained("./ner_model/checkpoint-125")
    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")
    print("‚úÖ NER –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    ner_pipeline = None

def extract_possible_titles(url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            print(f"[SKIP] {url}: Status code {response.status_code}")
            return [], True

        soup = BeautifulSoup(response.content, "lxml")
        candidates = []

        for tag in soup.find_all(['h1']):
            text = tag.get_text(strip=True)
            if text:
                candidates.append(text)

        for tag in soup.find_all(['p', 'div', 'span']):
            cls = tag.get("class")
            if cls and any(any(substr in c.lower() for substr in ["name", "title"]) for c in (cls if isinstance(cls, list) else [cls])):
                text = tag.get_text(strip=True)
                if text:
                    candidates.append(text)

        return candidates, False

    except requests.exceptions.SSLError as e:
        print(f"[SSL ERROR] {url}: {e}")
        return [], True
    except requests.exceptions.ConnectionError as e:
        print(f"[CONNECTION ERROR] {url}: {e}")
        return [], True
    except requests.exceptions.Timeout as e:
        print(f"[TIMEOUT ERROR] {url}: {e}")
        return [], True
    except requests.exceptions.HTTPError as e:
        print(f"[HTTP ERROR] {url}: {e}")
        return [], True
    except requests.exceptions.RequestException as e:
        print(f"[REQUEST ERROR] {url}: {e}")
        return [], True
    except Exception as e:
        print(f"[GENERAL ERROR] {url}: {e}")
        return [], True

def process_with_ner(texts):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ NER –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    if not ner_pipeline:
        return []
    
    results = []
    
    for text in texts:
        if not text or len(text.strip()) < 2:
            continue
            
        try:
            entities = ner_pipeline(text)
            
            for entity in entities:
                if entity.get('entity_group') == 'PRODUCT' or entity.get('entity') == 'PRODUCT':
                    confidence = entity.get('score', 0)
                    prob_percent = f"{int(confidence * 100)}%"
                    
                    results.append({
                        "text": text,
                        "prob": prob_percent
                    })
                    break  
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ '{text}': {e}")
            continue
    
    return results

@app.route('/api/analyze', methods=['POST'])
def analyze_url():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –º–µ–±–µ–ª–∏"""
    try:
        data = request.get_json()
        
        if not data or 'url' not in data:
            return jsonify({
                "error": True,
                "results": [],
                "products_identified": 0,
                "total_titles_found": 0,
                "message": "URL –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω"
            }), 400
        
        url = data['url']
        
        if not url.startswith(('http://', 'https://')):
            return jsonify({
                "error": True,
                "results": [],
                "products_identified": 0,
                "total_titles_found": 0,
                "message": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL"
            }), 400
        
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL: {url}")
        
        titles, scraping_error = extract_possible_titles(url)
        
        if scraping_error:
            return jsonify({
                "error": True,
                "results": [],
                "products_identified": 0,
                "total_titles_found": 0,
                "message": "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–∞"
            })
        
        if not titles:
            return jsonify({
                "error": False,
                "results": [],
                "products_identified": 0,
                "total_titles_found": 0,
                "message": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏—è —Å –¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
            })
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(titles)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        unique_titles = []
        seen = set()
        for title in titles:
            if title not in seen:
                unique_titles.append(title)
                seen.add(title)
        
        print(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_titles)} –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        results = process_with_ner(unique_titles)
        
        results.sort(key=lambda x: int(x['prob'].replace('%', '')), reverse=True)
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –º–µ–±–µ–ª–∏")
        
        return jsonify({
            "error": False,
            "results": results,
            "products_identified": len(results),
            "total_titles_found": len(unique_titles)
        })
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return jsonify({
            "error": True,
            "results": [],
            "products_identified": 0,
            "total_titles_found": 0,
            "message": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    model_status = "loaded" if ner_pipeline else "not_loaded"
    return jsonify({
        "status": "ok",
        "model_status": model_status
    })
    
@app.route('/', defaults={'path': ''})
@app.route('/<path:path>')
def serve_react(path):
    file_path = os.path.join(app.static_folder, path)
    if path != "" and os.path.exists(file_path):
        return send_from_directory(app.static_folder, path)
    else:
        return send_from_directory(app.static_folder, 'index.html')    

if __name__ == '__main__':
    print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º Flask —Å–µ—Ä–≤–µ—Ä...")
    print("üìù –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã:")
    print("  POST /api/analyze - –∞–Ω–∞–ª–∏–∑ URL –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –º–µ–±–µ–ª–∏")
    print("  GET /health - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞")
    
    app.run(debug=True, host='0.0.0.0', port=5000)